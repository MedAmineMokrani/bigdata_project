{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74e86307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "24/01/11 15:17:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import nbimporter\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, TimestampType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag \n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder \\\n",
    "        .appName(\"pyspark-notebook\") \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hive:9083\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\",\"/opt/hive/data/warehouse\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90643923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions.functions import read_table, create_hive_external_table\n",
    "\n",
    "def fetch_crypto_data_and_create_bronze_table(spark):\n",
    "\n",
    "    # Get the current date\n",
    "    current_date = datetime.date.today()\n",
    "\n",
    "    # Calculate the date 3 months ago\n",
    "    three_months_ago = current_date - datetime.timedelta(days=90)\n",
    "\n",
    "    # Create an empty list to store the timestamps in milliseconds\n",
    "    timestamps_milliseconds = []\n",
    "    data_json = []\n",
    "\n",
    "    # Initialize an index variable\n",
    "    index = 0\n",
    "\n",
    "    # Generate timestamps for every day between current_date and three_months_ago\n",
    "    date = current_date\n",
    "    while date >= three_months_ago:\n",
    "        timestamp_seconds = int(time.mktime(date.timetuple()))\n",
    "        timestamp_milliseconds = timestamp_seconds * 1000\n",
    "        timestamps_milliseconds.append(timestamp_milliseconds)\n",
    "        date -= datetime.timedelta(days=1)\n",
    "        index += 1\n",
    "\n",
    "\n",
    "\n",
    "    # Print the list of dates and their corresponding Unix timestamps in milliseconds\n",
    "    for i in range(len(timestamps_milliseconds) - 1):\n",
    "        current_timestamp = timestamps_milliseconds[i]\n",
    "        next_timestamp = timestamps_milliseconds[i + 1]\n",
    "\n",
    "        url_bitcoin = f\"https://api.coincap.io/v2/assets/bitcoin/history?interval=m1&start={next_timestamp}&end={current_timestamp}\"\n",
    "        url_ethereum = f\"https://api.coincap.io/v2/assets/ethereum/history?interval=m1&start={next_timestamp}&end={current_timestamp}\"\n",
    "\n",
    "\n",
    "        headers = {\n",
    "            'Authorization': 'Bearer 54a29da9-c63aa9-4627-8b13-756f453fb12c'\n",
    "        }\n",
    "\n",
    "\n",
    "        response_bitcoin = requests.get(url_bitcoin, headers=headers).json()\n",
    "        response_ethereum = requests.get(url_ethereum, headers=headers).json()\n",
    "\n",
    "\n",
    "\n",
    "        for data_element in response_bitcoin.get(\"data\", []):\n",
    "            data_element[\"currency\"] = \"bitcoin\"\n",
    "\n",
    "        for data_element in response_ethereum.get(\"data\", []):\n",
    "            data_element[\"currency\"] = \"ethereum\"\n",
    "\n",
    "        data_json.append(response_bitcoin)\n",
    "        data_json.append(response_ethereum)\n",
    "\n",
    "\n",
    "\n",
    "    df = spark.createDataFrame(data_json)\n",
    "    df = df.select(F.explode(df.data).alias(\"data\"))\n",
    "    df = df.select(F.col(\"data.date\"),F.col(\"data.time\"),F.col(\"data.priceUsd\"),F.col(\"data.circulatingSupply\"),F.col(\"data.currency\"))\n",
    "\n",
    "    create_hive_external_table(spark, df, \"bronze\", \"crypto_data\", \"currency\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db565c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/session.py:378: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n",
      "24/01/11 15:18:27 WARN TaskSetManager: Stage 0 contains a task of very large size (3624 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/01/11 15:18:39 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    }
   ],
   "source": [
    "fetch_crypto_data_and_create_bronze_table(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4372953",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc518245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
