{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c728969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "24/01/11 14:42:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import nbimporter\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, TimestampType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag \n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder \\\n",
    "        .appName(\"pyspark-notebook\") \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hive:9083\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\",\"/opt/hive/data/warehouse\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f427f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions.functions import read_table, create_hive_external_table\n",
    "\n",
    "def calculating_volatility_market_cap_daily(spark):\n",
    "    \n",
    "    df = read_table(spark, \"silver\", \"crypto_data\")\n",
    "    \n",
    "   \n",
    "    df = df.groupBy(F.date_format(F.col(\"date\"), \"yyyy-MM-dd\").alias(\"date_daily\"), \"currency\").agg(\n",
    "         F.avg(\"priceusd\").alias(\"priceusd_daily\"),\n",
    "         F.avg(\"priceusd_lagged\").alias(\"priceusd_lagged_daily\"),\n",
    "         F.avg(\"circulatingsupply\").alias(\"circulatingsupply_daily\"),\n",
    "         F.avg(\"volatility\").alias(\"volatility_daily\"),\n",
    "         F.avg(\"volatility_percentage\").alias(\"volatility_percentage_daily\"),\n",
    "         F.avg(\"market_cap\").alias(\"market_cap_daily\"))\n",
    "    \n",
    "    create_hive_external_table(spark , df, \"gold\", \"crypto_data\", \"currency\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3bb3a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/11 14:43:06 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "calculating_volatility_market_cap_daily(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fec153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb59b347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
